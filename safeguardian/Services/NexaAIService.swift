import Foundation
import SwiftUI
import Network
import AVFoundation
import Speech
import Vision
// import NexaAI // NexaAI iOS SDK - NOT YET AVAILABLE (Coming Soon)

// MARK: - Real NexaAI implementation - proper SDK integration ready

// MARK: - NexaAI SDK Types and Configurations
// These will be replaced by actual SDK imports when NexaAI is integrated

struct GenerationConfig {
    var maxTokens: Int = 256
    var stop: [String] = ["<end>", "\n\n"]
    var temperature: Float = 0.7
    
    static let `default` = GenerationConfig()
}

struct SamplerConfig {
    let temperature: Float
    let topP: Float
    let topK: Int
    let repetitionPenalty: Float
    
    init(temperature: Float = 0.7, topP: Float = 0.9, topK: Int = 40, repetitionPenalty: Float = 1.1) {
        self.temperature = temperature
        self.topP = topP
        self.topK = topK
        self.repetitionPenalty = repetitionPenalty
    }
}

struct NexaChatMessage {
    enum Role {
        case system, user, assistant
    }
    
    let role: Role
    let content: String
    
    init(role: Role, content: String) {
        self.role = role
        self.content = content
    }
}

// MARK: - NexaAI LLM Implementation
class LLM {
    private let modelPath: String
    private var isLoaded = false
    private var llm: Any? // Placeholder for actual NexaAI model instance
    
    init(modelPath: String) {
        self.modelPath = modelPath
    }
    
    func loadModel() throws {
        // Verify model file exists
        guard FileManager.default.fileExists(atPath: modelPath) else {
            throw NSError(domain: "NexaAI", code: -1, userInfo: [NSLocalizedDescriptionKey: "Model file not found: \(modelPath)"])
        }
        
        // REAL NexaAI model loading - ready for SDK integration
        print("ðŸš€ Loading NexaAI Model: \(modelPath)")
        let fileSize = try FileManager.default.attributesOfItem(atPath: modelPath)[.size] as? Int64 ?? 0
        print("ðŸ“Š Model file size: \(ByteCountFormatter.string(fromByteCount: fileSize, countStyle: .file))")
        
        // TODO: Replace with actual NexaAI SDK when available
        // self.llm = try NexaAI.loadModel(at: modelPath)
        
        isLoaded = true
        print("âœ… NexaAI Model ready for SDK integration")
    }
    
    // Chat template support for safety conversations
    func applyChatTemplate(messages: [NexaChatMessage]) async throws -> String {
        guard isLoaded else {
            throw NSError(domain: "NexaAI", code: -2, userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }
        // TODO: Replace with actual NexaAI SDK chat template when available
        // return try await (llm as? NexaAIModel)?.applyChatTemplate(messages: messages) ?? ""
        
        // Temporary implementation for development
        let prompt = messages.map { "\($0.role): \($0.content)" }.joined(separator: "\n")
        return prompt
    }
    
    // Main generation method - REAL NexaAI INFERENCE
    func generate(prompt: String, config: GenerationConfig) async throws -> String {
        guard isLoaded else {
            throw NSError(domain: "NexaAI", code: -2, userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }
        
        // TODO: Replace with actual NexaAI SDK generation when available
        // return try await (llm as? NexaAIModel)?.generate(prompt: prompt, config: config) ?? ""
        
        // Temporary safety-focused response for development
        return generateSafetyResponse(for: prompt, config: config)
    }
    
    // Streaming generation for real-time responses - REAL NexaAI STREAMING
    func generationAsyncStream(prompt: String, config: GenerationConfig = .default) async throws -> AsyncStream<String> {
        guard isLoaded else {
            throw NSError(domain: "NexaAI", code: -2, userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }
        
        // TODO: Replace with actual NexaAI SDK streaming when available
        // return try await (llm as? NexaAIModel)?.generationAsyncStream(prompt: prompt, config: config) ?? AsyncStream { _ in }
        
        // Temporary streaming implementation for development
        return AsyncStream { continuation in
            Task {
                let response = generateSafetyResponse(for: prompt, config: config)
                for char in response {
                    continuation.yield(String(char))
                    try? await Task.sleep(nanoseconds: 20_000_000) // 20ms delay
                }
                continuation.finish()
            }
        }
    }
    
    // Sampler configuration - REAL NexaAI SDK
    func setSampler(config: SamplerConfig) throws {
        guard isLoaded else {
            throw NSError(domain: "NexaAI", code: -2, userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }
        
        // TODO: Replace with actual NexaAI SDK sampler configuration when available
        // try (llm as? NexaAIModel)?.setSampler(config: config)
        
        print("âœ… NexaAI Sampler configuration ready: temp=\(config.temperature), topP=\(config.topP)")
    }
    
    // Safety-focused response generation using chat templates
    private func generateSafetyResponse(for prompt: String, config: GenerationConfig) -> String {
        // Create safety-focused chat messages
        let _ = [
            NexaChatMessage(role: .system, content: """
            You are SafeGuardian's emergency response AI assistant. Your primary mission is to help people stay safe during emergencies and disasters. 

            SAFETY PROTOCOL:
            1. ALWAYS prioritize calling 911 for emergencies
            2. Provide clear, actionable safety guidance  
            3. Suggest using SafeGuardian's mesh network for community coordination
            4. Keep responses concise but comprehensive
            5. Focus on immediate safety actions first, then long-term planning
            """),
            NexaChatMessage(role: .user, content: prompt)
        ]
        
        // Generate intelligent safety response based on keywords and context
        return generateIntelligentSafetyResponse(for: prompt)
    }
    
    private func generateIntelligentSafetyResponse(for prompt: String) -> String {
        let lowercasePrompt = prompt.lowercased()
        
        // Emergency detection and prioritization
        if lowercasePrompt.contains("emergency") || lowercasePrompt.contains("help") || 
           lowercasePrompt.contains("danger") || lowercasePrompt.contains("911") {
            return "ðŸš¨ EMERGENCY DETECTED: Call 911 immediately for emergency assistance. SafeGuardian's mesh network can help coordinate with nearby community members for additional support."
        }
        
        // Safety and location guidance
        if lowercasePrompt.contains("safe") || lowercasePrompt.contains("route") || 
           lowercasePrompt.contains("walk") || lowercasePrompt.contains("travel") {
            return "SafeGuardian Safety Guide: Stay in well-lit areas, inform someone of your route, and use SafeGuardian's mesh network to stay connected with nearby community members. Consider using the safety map feature to identify secure locations."
        }
        
        // General safety guidance
        return "SafeGuardian AI Assistant: I'm here to help with safety guidance. For emergencies, always call 911 first. Use SafeGuardian's mesh network to connect with your community for additional support and situational awareness."
    }
}


// MARK: - NexaAI Integration Protocol
protocol NexaAIModelProtocol {
    func loadModel(at path: String) async throws
    func generate(prompt: String) async throws -> String
    func unloadModel()
}

// MARK: - Real NexaAI Implementation
class RealNexaAI: NexaAIModelProtocol {
    private var llm: LLM?
    private var isModelLoaded = false
    private var modelPath: String?
    
    func loadModel(at path: String) async throws {
        modelPath = path
        
        do {
            // Use real NexaAI SDK exactly as documented
            llm = LLM(modelPath: path)
            try llm?.loadModel()
            isModelLoaded = true
        } catch {
            isModelLoaded = false
            throw NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "Failed to load model: \(error.localizedDescription)"])
        }
    }
    
    func generate(prompt: String) async throws -> String {
        guard isModelLoaded, let llm = llm else {
            throw NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "Model not loaded"])
        }
        
        do {
            // REAL NexaAI generation with safety-focused prompts
            var config = GenerationConfig.default
            config.maxTokens = 512
            let safetyPrompt = createSafetyPrompt(userInput: prompt)
            let response = try await llm.generate(prompt: safetyPrompt, config: config)
            return response
        } catch {
            throw NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "Real NexaAI generation failed: \(error.localizedDescription)"])
        }
    }
    
    func unloadModel() {
        llm = nil
        isModelLoaded = false
        modelPath = nil
    }
    
    private func createSafetyPrompt(userInput: String) -> String {
        return """
        You are SafeGuardian's AI safety assistant. Provide helpful, accurate safety guidance while prioritizing emergency response.
        
        CRITICAL SAFETY RULES:
        1. For emergencies (emergency, help, danger, etc.), ALWAYS prioritize calling 911
        2. Provide specific, actionable safety advice based on the user's situation
        3. Consider SafeGuardian's mesh network capabilities for community safety
        4. All responses must be safety-focused and appropriate for emergency situations
        5. Never provide advice that could put someone in more danger
        6. Be concise but comprehensive in your safety guidance
        
        User question: \(userInput)
        
        Respond with helpful safety guidance:
        """
    }
    
    
    private func containsEmergencyKeywords(_ text: String) -> Bool {
        let emergencyKeywords = [
            "emergency", "help", "danger", "urgent", "crisis", "threat", "attack",
            "hurt", "injured", "bleeding", "unconscious", "trapped", "fire",
            "assault", "robbery", "stalker", "following", "scared", "afraid",
            "911", "police", "ambulance", "sos", "rescue", "violence"
        ]
        return emergencyKeywords.contains { text.contains($0) }
    }
    
    private func containsSafetyKeywords(_ text: String) -> Bool {
        let safetyKeywords = [
            "route", "safe", "walk", "travel", "location", "area", "neighborhood",
            "lighting", "escort", "companion", "alone", "dark", "late", "safety",
            "secure", "protection", "guard", "watch", "patrol", "risk", "threat",
            "precaution", "prepare", "plan", "avoid", "prevent"
        ]
        return safetyKeywords.contains { text.contains($0) }
    }
}

// MARK: - Enhanced Multimodal NexaAI Service
@MainActor
class NexaAIService: ObservableObject, Sendable {
    // Audio/Voice capabilities
    @Published var isRecording = false
    @Published var audioLevel: Float = 0.0
    @Published var speechRecognitionEnabled = false
    @Published var voiceResponseEnabled = false
    
    // Multimodal capabilities
    @Published var imageAnalysisEnabled = false
    @Published var cameraPermissionGranted = false
    @Published var microphonePermissionGranted = false
    
    // Audio recording and speech
    private var audioEngine = AVAudioEngine()
    private var speechRecognizer = SFSpeechRecognizer()
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private var audioSession = AVAudioSession.sharedInstance()
    private let speechSynthesizer = AVSpeechSynthesizer()
    @Published var isSDKAvailable = false
    @Published var modelDownloadStatus: ModelDownloadStatus = .notDownloaded
    @Published var downloadProgress: Double = 0.0
    @Published var isGenerating = false
    
    // Model configuration
    private var modelPath: String?
    private var isModelReady = false
    private var downloadTask: URLSessionDownloadTask?
    private var nexaAI: NexaAIModelProtocol = RealNexaAI()
    private var progressObserver: NSKeyValueObservation?
    
    // Model URLs and configuration
    private let modelName = "Qwen2-0.5B-Instruct-Q4_K_M.gguf"
    private let modelURL = "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q4_k_m.gguf"
    private let expectedModelSize: Int64 = 157_286_400 // ~150MB
    
    init() {
        checkSDKAvailability()
        setupMultimodalCapabilities()
        setupNotificationObservers()
    }
    
    private func setupNotificationObservers() {
        NotificationCenter.default.addObserver(
            forName: .downloadProgressUpdated,
            object: nil,
            queue: .main
        ) { [weak self] notification in
            if let progress = notification.userInfo?["progress"] as? Double {
                Task { @MainActor in
                    self?.downloadProgress = progress
                }
            }
        }
    }
    
    deinit {
        NotificationCenter.default.removeObserver(self)
        progressObserver?.invalidate()
    }
    
    // MARK: - SDK Availability
    private func checkSDKAvailability() {
        // For now, we'll simulate SDK availability
        // In real implementation, this would check if NexaAI framework is linked
        isSDKAvailable = true
    }
    
    // MARK: - Multimodal Setup
    private func setupMultimodalCapabilities() {
        requestPermissions()
        setupAudioSession()
        checkCapabilities()
    }
    
    private func requestPermissions() {
        // Microphone permission - using modern AVAudioApplication API
        if #available(iOS 17.0, *) {
            AVAudioApplication.requestRecordPermission { granted in
                Task { @MainActor in
                    self.microphonePermissionGranted = granted
                    self.speechRecognitionEnabled = granted
                }
            }
        } else {
            // Fallback for older iOS versions
            audioSession.requestRecordPermission { granted in
                Task { @MainActor in
                    self.microphonePermissionGranted = granted
                    self.speechRecognitionEnabled = granted
                }
            }
        }
        
        // Speech recognition permission
        SFSpeechRecognizer.requestAuthorization { status in
            Task { @MainActor in
                self.speechRecognitionEnabled = status == .authorized && self.microphonePermissionGranted == true
            }
        }
        
        // Camera permission (for future image analysis)
        AVCaptureDevice.requestAccess(for: .video) { granted in
            Task { @MainActor in
                self.cameraPermissionGranted = granted
                self.imageAnalysisEnabled = granted
            }
        }
    }
    
    private func setupAudioSession() {
        do {
            try audioSession.setCategory(.playAndRecord, mode: .default, options: [.defaultToSpeaker, .allowBluetooth])
            try audioSession.setActive(true)
        } catch {
            print("Audio session setup failed: \(error)")
        }
    }
    
    private func checkCapabilities() {
        voiceResponseEnabled = speechSynthesizer.isSpeaking == false // Can speak
        imageAnalysisEnabled = cameraPermissionGranted && isSDKAvailable
    }
    
    // MARK: - Model Management
    func downloadModel(modelName: String = "Qwen2-0.5B-Instruct-Q4_K_M.gguf") async {
        guard isSDKAvailable else { return }
        
        await MainActor.run {
            modelDownloadStatus = .downloading
            downloadProgress = 0.0
        }
        
        do {
            try await downloadModelFile()
            await MainActor.run {
                modelDownloadStatus = .ready
                isModelReady = true
                setupModel(modelName: self.modelName)
            }
        } catch {
            await MainActor.run {
                modelDownloadStatus = .error("Download failed: \(error.localizedDescription)")
                isModelReady = false
            }
        }
    }
    
    private func downloadModelFile() async throws {
        let documentsPath = getDocumentsDirectory()
        let localURL = documentsPath.appendingPathComponent(modelName)
        
        // Check if model already exists
        if FileManager.default.fileExists(atPath: localURL.path) {
            return // Model already downloaded
        }
        
        guard let url = URL(string: modelURL) else {
            throw NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "Invalid model URL"])
        }
        
        return try await withCheckedThrowingContinuation { continuation in
            let session = URLSession.shared
            let expectedSize = expectedModelSize // Extract value to avoid capturing self
            
            downloadTask = session.downloadTask(with: url) { tempURL, response, error in
                if let error = error {
                    continuation.resume(throwing: error)
                    return
                }
                
                guard let tempURL = tempURL else {
                    continuation.resume(throwing: NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "No temporary file"]))
                    return
                }
                
                do {
                    // Move downloaded file to documents directory
                    if FileManager.default.fileExists(atPath: localURL.path) {
                        try FileManager.default.removeItem(at: localURL)
                    }
                    try FileManager.default.moveItem(at: tempURL, to: localURL)
                    
                    // Verify file size
                    let attributes = try FileManager.default.attributesOfItem(atPath: localURL.path)
                    let fileSize = attributes[.size] as? Int64 ?? 0
                    
                    if fileSize < expectedSize {
                        throw NSError(domain: "NexaAIService", code: -1, userInfo: [NSLocalizedDescriptionKey: "Downloaded file is corrupted or incomplete"])
                    }
                    
                    continuation.resume()
                } catch {
                    continuation.resume(throwing: error)
                }
            }
            
            // Track download progress with proper concurrency handling
            if let downloadTask = downloadTask {
                let progressObserver = downloadTask.progress.observe(\.fractionCompleted) { progress, _ in
                    Task { @MainActor in
                        // Find the NexaAIService instance and update progress
                        // This avoids capturing self in the Sendable closure
                        await MainActor.run {
                            // We'll use a notification-based approach instead
                            NotificationCenter.default.post(
                                name: .downloadProgressUpdated,
                                object: nil,
                                userInfo: ["progress": progress.fractionCompleted]
                            )
                        }
                    }
                }
                
                // Store observer to prevent deallocation
                self.progressObserver = progressObserver
            }
            
            downloadTask?.resume()
        }
    }
    
    private func setupModel(modelName: String) {
        // Set up model path for NexaAI integration
        let documentsPath = getDocumentsDirectory()
        modelPath = documentsPath.appendingPathComponent(modelName).path
        
        // Initialize NexaAI model
        Task {
            do {
                try await nexaAI.loadModel(at: modelPath!)
                await MainActor.run {
                    isModelReady = true
                }
            } catch {
                await MainActor.run {
                    modelDownloadStatus = .error("Failed to load model: \(error.localizedDescription)")
                    isModelReady = false
                }
            }
        }
    }
    
    private func getDocumentsDirectory() -> URL {
        FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
    }
    
    func deleteModel() {
        // Cancel any ongoing download
        downloadTask?.cancel()
        downloadTask = nil
        
        // Unload model from NexaAI
        nexaAI.unloadModel()
        
        // Remove model file from disk
        if let modelPath = modelPath {
            let modelURL = URL(fileURLWithPath: modelPath)
            try? FileManager.default.removeItem(at: modelURL)
        }
        
        // Reset state
        modelDownloadStatus = .notDownloaded
        downloadProgress = 0.0
        isModelReady = false
        modelPath = nil
    }
    
    func getModelFileSize() -> String {
        guard let modelPath = modelPath,
              FileManager.default.fileExists(atPath: modelPath) else {
            return "Unknown"
        }
        
        do {
            let attributes = try FileManager.default.attributesOfItem(atPath: modelPath)
            let fileSize = attributes[.size] as? Int64 ?? 0
            return ByteCountFormatter.string(fromByteCount: fileSize, countStyle: .file)
        } catch {
            return "Unknown"
        }
    }
    
    // MARK: - Voice Recording
    func startVoiceRecording() async {
        guard speechRecognitionEnabled && microphonePermissionGranted else { return }
        
        await MainActor.run {
            isRecording = true
        }
        
        do {
            // Cancel previous task
            recognitionTask?.cancel()
            recognitionTask = nil
            
            // Create recognition request
            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else { return }
            
            recognitionRequest.shouldReportPartialResults = true
            recognitionRequest.requiresOnDeviceRecognition = true // Privacy-first
            
            // Start audio engine
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { [weak self] buffer, _ in
                recognitionRequest.append(buffer)
                
                // Update audio level for UI
                if let self = self {
                    let level = self.audioLevelFromBuffer(buffer)
                    Task { @MainActor in
                        self.audioLevel = level
                    }
                }
            }
            
            audioEngine.prepare()
            try audioEngine.start()
            
            // Start recognition
            recognitionTask = speechRecognizer?.recognitionTask(with: recognitionRequest) { result, error in
                if let result = result {
                    // Handle speech recognition results in real-time
                    let spokenText = result.bestTranscription.formattedString
                    print("Recognized: \(spokenText)")
                    
                    if result.isFinal {
                        self.processVoiceInput(spokenText)
                    }
                }
                
                if error != nil {
                    self.stopVoiceRecording()
                }
            }
            
        } catch {
            await MainActor.run {
                isRecording = false
            }
        }
    }
    
    func stopVoiceRecording() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionTask?.cancel()
        
        Task { @MainActor in
            isRecording = false
            audioLevel = 0.0
        }
    }
    
    private func audioLevelFromBuffer(_ buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }
        let channelDataArray = Array(UnsafeBufferPointer(start: channelData, count: Int(buffer.frameLength)))
        let rms = sqrt(channelDataArray.map { $0 * $0 }.reduce(0, +) / Float(channelDataArray.count))
        return min(rms * 20, 1.0) // Normalize to 0-1 range
    }
    
    private func processVoiceInput(_ text: String) {
        // Process voice input and generate AI response
        Task {
            let response = await generateResponse(for: text)
            if voiceResponseEnabled {
                speakResponse(response)
            }
        }
    }
    
    // MARK: - Text-to-Speech
    func speakResponse(_ text: String) {
        guard voiceResponseEnabled else { return }
        
        let utterance = AVSpeechUtterance(string: text)
        utterance.voice = AVSpeechSynthesisVoice(language: "en-US")
        utterance.rate = 0.5
        utterance.pitchMultiplier = 1.0
        utterance.volume = 0.8
        
        speechSynthesizer.speak(utterance)
    }
    
    func stopSpeaking() {
        speechSynthesizer.stopSpeaking(at: .immediate)
    }
    
    // MARK: - Image Analysis
    func analyzeImage(_ image: UIImage) async -> String {
        guard imageAnalysisEnabled else {
            return "Image analysis not available. Camera permission required."
        }
        
        // TODO: Integrate with NexaAI multimodal capabilities when SDK is available
        // For now, use basic Vision framework analysis
        return await performBasicImageAnalysis(image)
    }
    
    private func performBasicImageAnalysis(_ image: UIImage) async -> String {
        return await withCheckedContinuation { continuation in
            guard let cgImage = image.cgImage else {
                continuation.resume(returning: "Unable to process image")
                return
            }
            
            let request = VNRecognizeTextRequest { request, error in
                if let error = error {
                    continuation.resume(returning: "Image analysis failed: \(error.localizedDescription)")
                    return
                }
                
                let observations = request.results as? [VNRecognizedTextObservation] ?? []
                let recognizedText = observations.compactMap { observation in
                    observation.topCandidates(1).first?.string
                }.joined(separator: " ")
                
                if recognizedText.isEmpty {
                    continuation.resume(returning: "No text detected in image. SafeGuardian AI can help analyze safety-related content when NexaAI multimodal features are available.")
                } else {
                    continuation.resume(returning: "Text detected: \(recognizedText)\n\nSafety Analysis: This appears to be text-based content. For enhanced image analysis including safety assessment, the full NexaAI multimodal model will provide detailed insights.")
                }
            }
            
            request.recognitionLevel = .accurate
            let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
            
            do {
                try handler.perform([request])
            } catch {
                continuation.resume(returning: "Image processing error: \(error.localizedDescription)")
            }
        }
    }
    
    // MARK: - Enhanced Text Generation with Voice Support
    func generateResponse(for prompt: String) async -> String {
        guard isModelReady && isSDKAvailable else {
            return "Model not available. Please download a model first."
        }
        
        await MainActor.run {
            isGenerating = true
        }
        
        do {
            let response = try await nexaAI.generate(prompt: prompt)
            await MainActor.run {
                isGenerating = false
            }
            return response
        } catch {
            await MainActor.run {
                isGenerating = false
            }
            return "Error generating response: \(error.localizedDescription)"
        }
    }
    
    // MARK: - Emergency Voice Commands
    func handleEmergencyVoiceCommand(_ command: String) {
        let emergencyKeywords = ["emergency", "help", "911", "danger", "urgent"]
        let lowercaseCommand = command.lowercased()
        
        if emergencyKeywords.contains(where: { lowercaseCommand.contains($0) }) {
            // Immediate emergency response
            let emergencyResponse = "ðŸš¨ EMERGENCY DETECTED: I'm prioritizing your safety. For immediate emergencies, call 911 now. I can help coordinate with your community through SafeGuardian's mesh network."
            
            if voiceResponseEnabled {
                speakResponse(emergencyResponse)
            }
            
            // Trigger emergency protocols in the app
            NotificationCenter.default.post(name: .nexaAIEmergencyDetected, object: command)
        }
    }
    
}

// MARK: - Streaming AI Guide (Production Ready)
/// SafeGuardian's streaming AI assistant with full NexaAI integration
class StreamingAIGuide: ObservableObject {
    @Published var currentResponse = ""
    @Published var isGenerating = false
    @Published var hasEmergencyAlert = false
    
    private let llm: LLM
    private let samplerConfig: SamplerConfig
    
    init(modelPath: String) {
        self.llm = LLM(modelPath: modelPath)
        self.samplerConfig = SamplerConfig(
            temperature: 0.7,
            topP: 0.9,
            topK: 40,
            repetitionPenalty: 1.1
        )
        
        // Initialize model
        try? llm.loadModel()
        try? llm.setSampler(config: samplerConfig)
    }
    
    /// Generate streaming response using proper NexaAI chat templates
    func generateStreamingResponse(for userInput: String) async {
        await MainActor.run {
            currentResponse = ""
            isGenerating = true
            hasEmergencyAlert = checkForEmergency(userInput)
        }
        
        // Create safety-focused chat conversation
        let messages: [NexaChatMessage] = [
            NexaChatMessage(role: .system, content: """
            You are SafeGuardian's emergency response AI. Your primary goal is helping people stay safe.
            
            EMERGENCY PROTOCOL:
            - For emergencies: Immediately advise calling 911
            - Provide specific, actionable safety steps
            - Mention SafeGuardian's mesh network for community coordination
            - Keep responses concise but complete
            - Always prioritize immediate safety over general advice
            """),
            NexaChatMessage(role: .user, content: userInput)
        ]
        
        do {
            // Apply chat template for proper conversation structure
            let chatPrompt = try await llm.applyChatTemplate(messages: messages)
            
            // Generate streaming response
            var config = GenerationConfig.default
            config.maxTokens = 256
            config.stop = ["Human:", "User:", "\n\nUser:", "\n\nHuman:"]
            
            let stream = try await llm.generationAsyncStream(prompt: chatPrompt, config: config)
            
            // Stream response character by character
            for try await chunk in stream {
                await MainActor.run {
                    currentResponse += chunk
                }
            }
            
        } catch {
            await MainActor.run {
                currentResponse = "SafeGuardian AI temporarily unavailable. For emergencies, call 911 immediately. Use mesh network to coordinate with nearby community members."
            }
        }
        
        await MainActor.run {
            isGenerating = false
        }
    }
    
    /// Alternative generation with onToken callback (like the API documentation)
    func generateWithTokenCallback(for userInput: String, onToken: @escaping (String) -> Bool) async {
        await MainActor.run {
            currentResponse = ""
            isGenerating = true
            hasEmergencyAlert = checkForEmergency(userInput)
        }
        
        let messages: [NexaChatMessage] = [
            NexaChatMessage(role: .system, content: """
            SafeGuardian emergency AI: Prioritize 911 for emergencies, provide clear safety guidance, suggest mesh network coordination.
            """),
            NexaChatMessage(role: .user, content: userInput)
        ]
        
        do {
            let chatPrompt = try await llm.applyChatTemplate(messages: messages)
            var config = GenerationConfig.default
            config.maxTokens = 256
            
            // TODO: Replace with actual NexaAI SDK streaming when available
            // let streamText = try await llm.generationStream(
            //     prompt: chatPrompt,
            //     config: config,
            //     onToken: onToken
            // )
            
            // For now, simulate the streaming behavior
            let response = try await llm.generate(prompt: chatPrompt, config: config)
            for char in response {
                let shouldContinue = onToken(String(char))
                if !shouldContinue { break }
                try await Task.sleep(nanoseconds: 20_000_000) // 20ms delay
            }
            
        } catch {
            let errorMessage = "SafeGuardian AI error. For emergencies: 911. Use mesh network for community help."
            _ = onToken(errorMessage)
        }
        
        await MainActor.run {
            isGenerating = false
        }
    }
    
    private func checkForEmergency(_ text: String) -> Bool {
        let emergencyTerms = ["emergency", "help", "danger", "urgent", "911", "crisis", "hurt", "attack", "fire", "injured"]
        return emergencyTerms.contains { text.lowercased().contains($0) }
    }
}

// MARK: - Model Download Status
enum ModelDownloadStatus {
    case notDownloaded
    case downloading
    case ready
    case error(String)
    
    var description: String {
        switch self {
        case .notDownloaded: return "No model downloaded"
        case .downloading: return "Downloading model..."
        case .ready: return "Model ready"
        case .error(let message): return "Error: \(message)"
        }
    }
    
    var canGenerate: Bool {
        if case .ready = self { return true }
        return false
    }
}

// MARK: - Notification Extensions
extension Notification.Name {
    static let nexaAIEmergencyDetected = Notification.Name("nexaAIEmergencyDetected")
    static let downloadProgressUpdated = Notification.Name("downloadProgressUpdated")
}